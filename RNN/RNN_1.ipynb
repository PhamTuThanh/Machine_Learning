{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9772589,"sourceType":"datasetVersion","datasetId":5986064}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\n\nclass DataProcessor:\n    def __init__(self, num_words=10000, oov_token=\"<OOV>\", max_length=100, padding_type='post', truncating_type='post'):\n        self.tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n        self.max_length = max_length\n        self.padding_type = padding_type\n        self.truncating_type = truncating_type\n    \n    def fit_tokenizer(self, texts):\n        self.tokenizer.fit_on_texts(texts)\n    \n    def text_to_sequences(self, texts):\n        return self.tokenizer.texts_to_sequences(texts)\n    \n    def pad_sequences(self, sequences):\n        return pad_sequences(sequences, maxlen=self.max_length, padding=self.padding_type, truncating=self.truncating_type)\n    \n    def process_texts(self, texts):\n        sequences = self.text_to_sequences(texts)\n        padded_sequences = self.pad_sequences(sequences)\n        return padded_sequences\n\n    def get_word_index(self):\n        return self.tokenizer.word_index\n\n    def save_tokenizer(self, path='tokenizer.json'):\n        tokenizer_json = self.tokenizer.to_json()\n        with open(path, 'w') as f:\n            f.write(tokenizer_json)\n\n    def load_tokenizer(self, path='tokenizer.json'):\n        with open(path) as f:\n            tokenizer_json = f.read()\n        self.tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_json)\n\n# Sử dụng DataProcessor để xử lý dữ liệu\ndata_processor = DataProcessor()\n\n# Giả sử chúng ta có một số dữ liệu văn bản\ntexts = [\"Hello, how are you?\", \"I am fine, thank you!\", \"What about you?\"]\n\n# Huấn luyện tokenizer\ndata_processor.fit_tokenizer(texts)\n\n# Chuyển đổi văn bản thành chuỗi số\nsequences = data_processor.text_to_sequences(texts)\n\n# Pad sequences\npadded_sequences = data_processor.pad_sequences(sequences)\n\nprint(\"Padded Sequences:\")\nprint(padded_sequences)\n\n# Lưu tokenizer để sử dụng sau\ndata_processor.save_tokenizer()\n\n# Tải lại tokenizer\nnew_data_processor = DataProcessor()\nnew_data_processor.load_tokenizer()\n\n# Kiểm tra lại các chỉ số từ\nprint(\"Word Index:\")\nprint(new_data_processor.get_word_index())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T13:47:12.102955Z","iopub.execute_input":"2024-10-31T13:47:12.103381Z","iopub.status.idle":"2024-10-31T13:47:12.124562Z","shell.execute_reply.started":"2024-10-31T13:47:12.103343Z","shell.execute_reply":"2024-10-31T13:47:12.123039Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Padded Sequences:\n[[ 3  4  5  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0]\n [ 6  7  8  9  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0]\n [10 11  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0]]\nWord Index:\n{'<OOV>': 1, 'you': 2, 'hello': 3, 'how': 4, 'are': 5, 'i': 6, 'am': 7, 'fine': 8, 'thank': 9, 'what': 10, 'about': 11}\n","output_type":"stream"}]}]}